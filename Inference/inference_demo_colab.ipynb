{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Bias Detection in Image Captions\n",
    "\n",
    "This notebook demonstrates how to use our fine-tuned LlamaV-o1 model to detect gender bias in image-caption pairs. The model has been trained to:\n",
    "1. Analyze an image and its caption\n",
    "2. Generate a step-by-step chain-of-thought reasoning\n",
    "3. Provide a final judgment (Biased or Not Biased)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, we'll install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate pillow matplotlib sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Model\n",
    "\n",
    "Now we'll load our fine-tuned LlamaV-o1 model. You have two options:\n",
    "1. Load from a local path (if you have the model files)\n",
    "2. Load from Hugging Face Hub (if the model is published there)\n",
    "\n",
    "Let's implement both options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, CLIPProcessor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Choose which option to use\n",
    "use_local_model = False  # Set to True to use local model files\n",
    "\n",
    "# Option 1: Load from local path\n",
    "if use_local_model:\n",
    "    model_path = \"path/to/fine_tuned_model\"  # Change this to your model path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    )\n",
    "    # Also load the image processor\n",
    "    image_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    \n",
    "# Option 2: Load from Hugging Face Hub\n",
    "else:\n",
    "    # Replace with your actual model ID if published\n",
    "    model_id = \"your-username/llamav-o1-gender-bias-detector\"\n",
    "    \n",
    "    # For this demo, we'll use the base LlamaV-o1 model (not fine-tuned)\n",
    "    # In a real scenario, you would use your fine-tuned model\n",
    "    model_id = \"mbzuai-oryx/LlamaV-o1-7B\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    )\n",
    "    # Load the image processor\n",
    "    image_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Ensure the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Inference Function\n",
    "\n",
    "Now we'll define a function to run inference on image-caption pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_bias(image, caption, max_new_tokens=512):\n",
    "    \"\"\"Analyze gender bias in an image-caption pair.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or path to image\n",
    "        caption: str, the caption to analyze\n",
    "        max_new_tokens: int, maximum number of tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        dict with reasoning and bias label\n",
    "    \"\"\"\n",
    "    # Load the image if it's a path\n",
    "    if isinstance(image, str):\n",
    "        if image.startswith(\"http\"):\n",
    "            response = requests.get(image)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        else:\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "    \n",
    "    # Process the image\n",
    "    pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "    if torch.cuda.is_available():\n",
    "        pixel_values = pixel_values.cuda()\n",
    "    \n",
    "    # Prepare the prompt\n",
    "    prompt = f\"Analyze this image and caption for gender bias: \\\"{caption}\\\"\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # Run inference\n",
    "    # Note: This is a simplified inference approach\n",
    "    # For LlamaV-o1, you would need to modify this to match the model's specific input format\n",
    "    # including how to pass the image features\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "        )\n",
    "    \n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the reasoning and bias label\n",
    "    result_text = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    # Parse the result to extract reasoning and bias label\n",
    "    # This is a simplified approach - in a real scenario, you might need more robust parsing\n",
    "    if \"Biased\" in result_text.split(\"\\n\")[-1]:\n",
    "        bias_label = \"Biased\"\n",
    "    elif \"Not Biased\" in result_text.split(\"\\n\")[-1]:\n",
    "        bias_label = \"Not Biased\"\n",
    "    else:\n",
    "        # Default if we can't clearly determine\n",
    "        bias_label = \"Unclear\"\n",
    "    \n",
    "    reasoning = result_text\n",
    "    \n",
    "    return {\n",
    "        \"reasoning\": reasoning,\n",
    "        \"bias_label\": bias_label\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example 1: Potentially Biased Caption\n",
    "\n",
    "Let's analyze a potentially biased image-caption pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 1: Potentially biased caption\n",
    "image_url_1 = \"https://example.com/path/to/image1.jpg\"  # Replace with a real image URL\n",
    "caption_1 = \"A woman in the kitchen preparing dinner while her husband relaxes in the living room.\"\n",
    "\n",
    "# For demonstration purposes with a real image URL\n",
    "# (replace with your own example URL)\n",
    "image_url_1 = \"https://images.unsplash.com/photo-1556911220-bff31c812dba\"\n",
    "\n",
    "# Display the image\n",
    "image_1 = Image.open(BytesIO(requests.get(image_url_1).content)).convert(\"RGB\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image_1)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Caption: {caption_1}\")\n",
    "plt.show()\n",
    "\n",
    "# Analyze for bias\n",
    "result_1 = analyze_bias(image_1, caption_1)\n",
    "\n",
    "# Display results\n",
    "print(\"✨ Analysis Results ✨\")\n",
    "print(f\"Caption: {caption_1}\")\n",
    "print(f\"\\nBias Label: {result_1['bias_label']}\")\n",
    "print(\"\\nReasoning:\")\n",
    "print(result_1['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example 2: Non-Biased Caption\n",
    "\n",
    "Now let's analyze a likely non-biased image-caption pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 2: Non-biased caption\n",
    "image_url_2 = \"https://example.com/path/to/image2.jpg\"  # Replace with a real image URL\n",
    "caption_2 = \"Two scientists working together in a laboratory on a chemical experiment.\"\n",
    "\n",
    "# For demonstration purposes with a real image URL\n",
    "# (replace with your own example URL)\n",
    "image_url_2 = \"https://images.unsplash.com/photo-1532094349884-543bc11b234d\"\n",
    "\n",
    "# Display the image\n",
    "image_2 = Image.open(BytesIO(requests.get(image_url_2).content)).convert(\"RGB\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image_2)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Caption: {caption_2}\")\n",
    "plt.show()\n",
    "\n",
    "# Analyze for bias\n",
    "result_2 = analyze_bias(image_2, caption_2)\n",
    "\n",
    "# Display results\n",
    "print(\"✨ Analysis Results ✨\")\n",
    "print(f\"Caption: {caption_2}\")\n",
    "print(f\"\\nBias Label: {result_2['bias_label']}\")\n",
    "print(\"\\nReasoning:\")\n",
    "print(result_2['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Upload Your Own Image and Caption\n",
    "\n",
    "Now you can try with your own image and caption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Upload an image\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the filename of the uploaded image\n",
    "uploaded_filename = list(uploaded.keys())[0]\n",
    "user_image = Image.open(uploaded_filename).convert(\"RGB\")\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(user_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Enter a caption\n",
    "user_caption = input(\"Enter a caption for the image: \")\n",
    "\n",
    "# Analyze for bias\n",
    "user_result = analyze_bias(user_image, user_caption)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n✨ Analysis Results ✨\")\n",
    "print(f\"Caption: {user_caption}\")\n",
    "print(f\"\\nBias Label: {user_result['bias_label']}\")\n",
    "print(\"\\nReasoning:\")\n",
    "print(user_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to use our fine-tuned LlamaV-o1 model to analyze image-caption pairs for gender bias. The model provides:\n",
    "\n",
    "1. A chain-of-thought reasoning process that examines various aspects of potential bias\n",
    "2. A final judgment (Biased, Not Biased, or Unclear)\n",
    "\n",
    "This approach can help identify and address gender bias in image descriptions, contributing to more inclusive and fair visual content.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Note that this model has several limitations:\n",
    "- It's trained on a specific dataset and may not generalize to all types of bias\n",
    "- The judgment is based on the fine-tuning data, which reflects certain perspectives\n",
    "- The model may sometimes produce unclear or inconsistent reasoning\n",
    "\n",
    "Always use human judgment alongside the model's analysis for important decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
